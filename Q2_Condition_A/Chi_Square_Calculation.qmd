---
title: "Applying Chi-Square to Categorical Data Tutorial"
format: 
  html:
      embed-resources: true
      toc: true
editor: visual
date-modified: 14/06/2023
---

# Introduction

Many of the statistical tests you learn about in psychology research methods focus on continuous outcomes. While many measures capture continuous data, that is not the only way to quantify human behaviour. Imagine you performed an observational study where you coded for whether a baby exhibited one type of behaviour or another. This would represent categorical or nominal data and would not be suitable to use in something like a t-test. In this tutorial, you will learn about applying the **Chi-Square test of independence** to categorical outcomes.

When analysing categorical data, the most informative summary statistic is the mode. We can calculate the frequency of each level or category in the variable, and the most frequent is our modal value. This means a bar plot is also the most direct way of summarising the data as they are designed to show the frequency of different categories. For example, in the `mtcars` data set, we can plot the transmission type of cars included in the 1974 edition of *Motor Trend* magazine. 19 were automatic and 13 were manual, where the automatic cars was the modal category. This shows how we can summarise categorical data.

```{r firstbarplot, echo=FALSE, warning=FALSE, message=FALSE}

library(ggplot2)
library(webexercises)

ggplot(aes(x = as.factor(am)), data = mtcars) + 
  geom_bar(stat = "count") + 
  theme_minimal() + 
  scale_y_continuous(limits = c(0,20), breaks = seq(0, 20, 5), name = "Frequency") + 
  scale_x_discrete(labels = c("Automatic", "Manual"), name = "Type of Transmission") + 
  labs(title = "The Frequency of Transmission Type in Motor Trend Car Road Tests")

```

The Chi-Square ($\chi^2$) tests whether there is an association in categorical variables. A **one-sample Chi-Square** can be applied to two or more categories of a single variable, whereas a **cross-tabulation Chi-Square** can be applied to two or more categorical variables, each containing two or more levels. We are interested in whether the observed categorical frequency distribution differs to what we would expect assuming there was no association.

In the following tutorial, we will demonstrate how to conduct a one-sample and cross-tabulation Chi-Square test in R. We will first explain the logic and assumptions behind each test, then how you can calculate the test in R and report your findings.

We assume you have a basic understanding of R as we are focusing on the data preparation and analysis steps, rather than explaining the R language, but that you have never performed a Chi-Square test in R before. We also assume that you are familiar with the null hypothesis significance testing framework (NHST) for statistical inference.

To follow this tutorial, you only need the `tidyverse` collection of packages loaded for wrangling and plotting functions.

```{r load packages, message=FALSE, warning=FALSE}

library(tidyverse)

```

# One-Sample Chi-Square

## Overview and assumptions

In the **one-sample Chi-Square** test, you have a single categorical variable with two or more levels. For the test's assumptions, the data should also be independent, meaning each participant or case provides just one observation. Chi-Square tests are not designed for repeated measures data. There is an alternative called McNemar's test that is designed for repeated measures data, but we will not be covering this test here.

We start with **observed values** which are your data. For example, imagine you conducted an observational study where you recorded whether people passing in the street returned a smile or not. You recorded 25 people who returned a smile and 37 people who did not return a smile. These are your observed values. We compare these to our **expected values** which is what we would expect if there was equal likelihood of returning a smile or not: 31 and 31.

We then apply null hypothesis significance testing logic to compare your observed values to your expected values. Chi-Square is the test statistic that represents this process. The larger the difference between your observed and expected values, the larger the test statistic and the more likely the test will be statistically significant (assuming the sample size remained the same).

Traditionally, you would check a look-up table for the intersection between your Chi-Square test statistic and the degrees of freedom. For the one-sample Chi-Square, the degrees of freedom are $k - 1$, where k means the number of categories, which would be 1 in this scenario (2 - 1). If your Chi-Square test statistic is equal to or greater than the critical value for your alpha (traditionally $\alpha$ = .05), then you would reject the null hypothesis and conclude the test is statistically significant. If your test statistic is smaller than the critical value, then you would not reject the null hypothesis and the test would not be statistically significant.

For a selection of degrees of freedom, the critical values are produced below. For our example, it corresponds to a Chi-Square value of 2.32 (which we will verify shortly). Since we have 1 degrees of freedom, the critical value for our Chi-Square is 3.84. Since our Chi-Square is smaller than the critical value, there is not a statistically significant association at $\alpha$ = .05.

### Chi-Square ($\chi^2$) Look-up Table

| df  |                    $\alpha = .05$                     |
|:---:|:-----------------------------------------------------:|
|  1  | `r qchisq(.05, 1, lower.tail = FALSE) %>% round2(3)`  |
|  2  | `r qchisq(.05, 2, lower.tail = FALSE) %>% round2(3)`  |
|  3  | `r qchisq(.05, 3, lower.tail = FALSE) %>% round2(3)`  |
|  4  | `r qchisq(.05, 4, lower.tail = FALSE) %>% round2(3)`  |
|  5  | `r qchisq(.05, 5, lower.tail = FALSE) %>% round2(3)`  |
|  6  | `r qchisq(.05, 6, lower.tail = FALSE) %>% round2(3)`  |
|  7  | `r qchisq(.05, 7, lower.tail = FALSE) %>% round2(3)`  |
|  8  | `r qchisq(.05, 8, lower.tail = FALSE) %>% round2(3)`  |
|  9  | `r qchisq(.05, 9, lower.tail = FALSE) %>% round2(3)`  |
| 10  | `r qchisq(.05, 10, lower.tail = FALSE) %>% round2(3)` |

Now we have computers and statistics software, we do not need to look at a critical values table, R will directly calculate the *p*-value for your test statistic and degrees of freedom. You then interpret whether the *p*-value is smaller or larger than your alpha, and conclude whether the test is statistically significant or not.

## Hand calculations

```{r, message=FALSE, warning=FALSE, echo=FALSE}

dat_o <- tibble(Conditions = c("S","NS"),
                Counts = c(25, 37)) %>% 
  pivot_wider(names_from = "Conditions", values_from = "Counts") %>%
  mutate(Values = "Observed") %>%
  select(Values, everything())

md_d <- dat_o %>% 
  pivot_longer(cols = S:NS,
               names_to = "Conditions", 
               values_to = "Counts") %>% 
  pull(Counts)

colsMax <- which(md_d == max(md_d))

```

Although we now have computers to do the hard work for us, it is still important to work through the underlying hand calculations - even if it is just once - to appreciate where the values come from. In this part, we will work through the smiling observation data and how we can calculate the one-sample Chi-Square by hand. Here is our data:

```{r we-table-orig, echo=FALSE}
knitr::kable(dat_o, align = "c")
```

And if we add on a column showing the total number of participants, adding all the numbers in the different conditions together, (i.e. `r dat_o %>% pull(S)` + `r dat_o %>% pull(NS)` = `r dat_o %>% pull(S)+ dat_o %>% pull(NS)`), then we get:

```{r we-table-totals, echo=FALSE}
dat <- dat_o %>% 
  mutate(Total = S + NS)

knitr::kable(dat, align = "c")
```

Now the formula for the chi-square is:

$$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$$

The Expected values for each condition, in a one-sample chi-square assuming a uniform (equal) distribution is calculated by $N \times \frac{1}{k}$ where $k$ is the number of conditions and $N$ is the total number of participants. This can also be written more straightforward as $N/k$. That means that in our example the expected value in each condition would be:

$$Expected = \frac{N}{k} = \frac{`r dat %>% pull(Total)`}{`r dim(dat)[2] - 2`} = `r dat %>% pull(Total)/(dim(dat)[2] - 2)`$$

Let's now add those Expected values to our table which looks like:

```{r we-table-expecteds, echo=FALSE}
expecteds <- tibble(S = dat %>% pull(Total)/(dim(dat)[2] - 2),
                    NS = dat %>% pull(Total)/(dim(dat)[2] - 2)) %>%
  mutate(Total = S + NS,
         Values = "Expected") %>%
  select(Values, everything())

dat_e <- bind_rows(dat, expecteds)

knitr::kable(dat_e, align = "c")
```

We now have our data, let's start putting it into the formula, which we said was:

$$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$$

Which really means:

$$\chi^2 = \frac{(Observed_{A} - Expected_{A})^2}{Expected_{A}} + \frac{(Observed_{B} - Expected_{B})^2}{Expected_{B}}$$

So

$$\chi^2 = \frac{(`r dat_e %>% filter(Values == "Observed") %>% pull(S)` - `r dat_e %>% filter(Values == "Expected") %>% pull(S)`)^2}{`r dat_e %>% filter(Values == "Expected") %>% pull(S)`}+\frac{(`r dat_e %>% filter(Values == "Observed") %>% pull(NS)` - `r dat_e %>% filter(Values == "Expected") %>% pull(NS)`)^2}{`r dat_e %>% filter(Values == "Expected") %>% pull(NS)`}$$

Which becomes:

$$\chi^2 = \frac{(`r dat_e %>% filter(Values == "Observed") %>% pull(S) - dat_e %>% filter(Values == "Expected") %>% pull(S)`)^2}{`r dat_e %>% filter(Values == "Expected") %>% pull(S)`} + \frac{(`r dat_e %>% filter(Values == "Observed") %>% pull(NS) - dat_e %>% filter(Values == "Expected") %>% pull(NS)`)^2}{`r dat_e %>% filter(Values == "Expected") %>% pull(NS)`}$$

And if we now square the top halves (the numerators):

$$\chi^2 = \frac{`r (dat_e %>% filter(Values == "Observed") %>% pull(S) - dat_e %>% filter(Values == "Expected") %>% pull(S))^2`}{`r dat_e %>% filter(Values == "Expected") %>% pull(S)`} + \frac{`r (dat_e %>% filter(Values == "Observed") %>% pull(NS) - dat_e %>% filter(Values == "Expected") %>% pull(NS))^2`}{`r dat_e %>% filter(Values == "Expected") %>% pull(NS)`}$$

Then divide the top half by the bottom half for each condition:

$$\chi^2 = {`r (dat_e %>% filter(Values == "Observed") %>% pull(S) - dat_e %>% filter(Values == "Expected") %>% pull(S))^2/dat_e %>% filter(Values == "Expected") %>% pull(S)`}+{`r (dat_e %>% filter(Values == "Observed") %>% pull(NS) - dat_e %>% filter(Values == "Expected") %>% pull(NS))^2/dat_e %>% filter(Values == "Expected") %>% pull(NS)`}$$ And finally add them altogether

$$\chi^2 = `r ((dat_e %>% filter(Values == "Observed") %>% pull(S) - dat_e %>% filter(Values == "Expected") %>% pull(S))^2/dat_e %>% filter(Values == "Expected") %>% pull(S))  +  ((dat_e %>% filter(Values == "Observed") %>% pull(NS) - dat_e %>% filter(Values == "Expected") %>% pull(NS))^2/dat_e %>% filter(Values == "Expected") %>% pull(NS))`$$

So we find that $\chi^2 = `r ((dat_e %>% filter(Values == "Observed") %>% pull(S) - dat_e %>% filter(Values == "Expected") %>% pull(S))^2/dat_e %>% filter(Values == "Expected") %>% pull(S)) + ((dat_e %>% filter(Values == "Observed") %>% pull(NS) - dat_e %>% filter(Values == "Expected") %>% pull(NS))^2/dat_e %>% filter(Values == "Expected") %>% pull(NS))`$

The degrees of freedom in this test is $k - 1$ and given that we have `r dim(dat_o)[2]-1` conditions:

$$df = k - 1$$ $$df = `r dim(dat_o)[2]-1` - 1$$ $$df = `r (dim(dat_o)[2]-1) - 1`$$

Applying the NHST framework, there is a Chi-Square distribution which depends on the value for $k$. Isolating the distribution for $k = 2$ and assuming $\alpha = .05$, we can summarise the procedure with the figure below. Using $\alpha = .05$ creates a critical value of 3.84, but our observed test statistic of 2.32 does not exceed that threshold, meaning the test is not statistically significant and we cannot reject the null hypothesis.

```{r Chi Square figure, echo=FALSE}
knitr::include_graphics("Images/Chi_Square_Dist.png")
```

If you compare these values to the overview, they match up and demonstrate the underlying hand calculations. If you check back at the look-up table, our conclusion is still the association is still not statistically significant.

## Calculating in R

Now you understand the logic behind the test, we can demonstrate how you perform the one-sample Chi-Square in R.

There are two main ways you will have data to work with. The first is raw values of observations, such as when you have a complete data set and one column represents a categorical variable with two or more levels. The second is when you have a table of frequencies for each level of your categorical variable. To apply the Chi-Square, you first need a frequency table to work with, so we will demonstrate the wrangling process in case you are starting with raw values.

### Wrangling a frequency table

If you are working with raw data, you first need to arrange your categories into a frequency table. Let us create some mock data to work from using the smile example.

```{r Create raw smiles data}

Smiles <- data.frame(ID = 1:62,
                     response = rep(c("Smile", "No Smile"), # Our two categories
                                    c(25, 37))) # Repeat smile 25 times, no smile 37 times

head(Smiles)
```

We created 62 rows of data, 25 for participants who returned a smile, and 37 for those who did not return a smile. The next step is to calculate the frequency of each category.

We can easily calculate the frequency by isolating the response variable for our categories, and applying the `table()` function from the base R set of functions. This function creates a contingency table of all the variables you provide, which will scale nicely once we turn to the cross-tabulation Chi-Square later. With one variable, it will just count how many observations of each category there are.

```{r Count smiles data}

Smiles_frequency <- Smiles %>% 
  select(response) %>% 
  table()

Smiles_frequency
```

### Entering data as a frequency table

Alternatively, you might immediately start with a frequency table if you do not have access to the raw data. In this case, you can directly enter the values for each category. There are different ways of entering data, but the most direct is entering values as a matrix and adding informative names.

In the code below, we first save a matrix with the frequencies for each category. The other argument controls how to organise the matrix like how many columns it has.

```{r Frequency table}
Smiles_frequency <- matrix(c(25, 37), # Frequency values for each category
                           ncol = 2, # 2 columns, so the data are not in one column
                           byrow = TRUE) # Enter the values one row at a time

# For clarity, add names to each column
colnames(Smiles_frequency) <- c("Smile", "No Smile")

Smiles_frequency
```

### Chi-Square function in R

No matter which method you used to enter your data, it will be in the same form and ready to analyse. R comes with a built-in function for applying the Chi-Square test called `chisq.test()`. All you need to do is enter your frequency object you created before.

```{r chi square smiles}

chisq.test(Smiles_frequency)

```

To break down the output, the function confirms the data source you used. You get the Chi-Square value, degrees of freedom, and the *p*-value. Reassuringly, all these values correspond with what we reported in the overview. There is not a statistically significant association here and we cannot reject the null hypothesis. If you want to communicate the results in a report, the standardised APA format is:

```{r chi square smiles reproducible, echo=FALSE}

chi_square <- chisq.test(Smiles_frequency)

CQ_stat <- round(chi_square$statistic, 2)
CQ_df <- chi_square$parameter

CQ_pval <- round(chi_square$p.value, 3)
CQ_pval <- substr(as.character(CQ_pval), 2, 5)

```

$\chi^2$ (`r CQ_df`) = `r CQ_stat`, *p* = `r CQ_pval`.

For any analysis, data visualisation is important and for a Chi-Square, a bar plot is the most appropriate choice to show the frequency of values across different categories. For this data set, you can use `ggplot2` to create a bar plot showing the frequency of each smile observation.

```{r smile bar plot}
Smiles %>% 
  ggplot(aes(x = response)) + 
  geom_bar() + 
  theme_minimal() + 
  scale_y_continuous(limits = c(0,40), breaks = seq(0, 40, 5), name = "Frequency") + 
  scale_x_discrete(name = "Type of Facial Response") + 
  labs(title = "The Frequency of Whether Participants Returned a Smile or Not.")
```

There is also extra information available in the function which can be helpful for a final assumption we have not covered so far. To work as intended, the expected values must be higher than 5. If the expected value is less than 5 for one of the cells/categories, you will receive a warning that the results may not be accurate. There are different guidelines around how many cells with fewer than 5 expected values are too few, but one recommendation is at least 80% of the cells should have 5 or more expected values. For example, we can check the expected values for the current example:

```{r full expected values}
chisq.test(Smiles_frequency)$expected
```

To demonstrate what the warning looks like, we will need to amend the data set so there are fewer than 5 expected values.

```{r five expected values}

Smiles_frequency <- matrix(c(2, 7), # Smaller number of observed values
                           ncol = 2,
                           byrow = TRUE) 

colnames(Smiles_frequency) <- c("Smile", "No Smile")

chisq.test(Smiles_frequency)

chisq.test(Smiles_frequency)$expected
```

This shows how when you have fewer than 5 expected values, you receive the warning "Chi-squared approximation may be incorrect". This highlights how the test statistic and *p*-value may be inaccurate when there is too little data to work with.

When it comes to the cross-tabulation Chi-Square, there is an alternative test designed for small samples called Fisher's exact test, but you need at least 2 rows and columns for it to work. Therefore, for the one-sample Chi-Square, it essentially presents a guide for the minimal sample size to work with. We will return to Fisher's exact test as an alternative at the end of the next section.

# Cross-Tabulation Chi-Square

## Overview and assumptions

Now you have worked through the one-sample Chi-Square, it is time to demonstrate how it scales up. In the **cross-tabulation Chi-Square** test, you have a two or more categorical variables, each with two or more levels. For the test's assumptions, the data should be independent, meaning each participant or case provides just one observation.

For this example, we will add a variable to our observational study. Imagine in addition to recording whether people passing in the street returned a smile or not, we also recorded the weather on testing days. We recorded whether it was rainy or sunny, and we want to know whether there is an association between the weather and people's facial responses. This represents a 2x2 design as we have two categorical variables, each with two levels.

For our **observed values**, on a rainy day we recorded 25 people returning a smile and 37 people not returning a smile. On a sunny day, we then recorded 50 people returning a smile and 33 not returning a smile. The cross-tabulation Chi-Square still works by comparing our observed values to our **expected values**. For this example, they are 32.07, 29.93, 42.93, and 40.07 respectively. Calculating the expected values are a little more involved than the one-sample Chi-Square, so we will verify the values later in the tutorial and focus on the overall logic for now.

We then apply null hypothesis significance testing logic to compare your observed values to your expected values. Chi-Square is the test statistic that represents this process. Traditionally, you would check a look-up table for the intersection between your Chi-Square test statistic and the degrees of freedom.

For the cross-tabulation Chi-Square, the degrees of freedom are:

$$df = (Rows - 1) \times (Columns - 1)$$

Rows are the number of categories for one variable and columns for the number of categories in your other variable. In this example, the value for the degrees of freedom is 1 (2-1 x 2-1 = 1). If your Chi-Square test statistic is equal to or greater than the critical value for your alpha (traditionally $\alpha$ = .05), then you would reject the null hypothesis and conclude the test is statistically significant. If your test statistic is smaller than the critical value, then you would not reject the null hypothesis and the test would not be statistically significant.

For our example, it corresponds to a Chi-Square value of 5.64 (which we will verify shortly). Since we have 1 degrees of freedom, the critical value for our Chi-Square is 3.84 and we have reproduced a truncated version of the Chi-Square look-up table below. Since our Chi-Square is larger than the critical value, there is a statistically significant association at $\alpha$ = .05 and we can reject the null hypothesis.

### Chi-Square ($\chi^2$) Look-up Table

| df  |                    $\alpha = .05$                    |
|:---:|:----------------------------------------------------:|
|  1  | `r qchisq(.05, 1, lower.tail = FALSE) %>% round2(3)` |
|  2  | `r qchisq(.05, 2, lower.tail = FALSE) %>% round2(3)` |
|  3  | `r qchisq(.05, 3, lower.tail = FALSE) %>% round2(3)` |
|  4  | `r qchisq(.05, 4, lower.tail = FALSE) %>% round2(3)` |
|  5  | `r qchisq(.05, 5, lower.tail = FALSE) %>% round2(3)` |

## Hand calculations

```{r, message=FALSE, warning=FALSE, echo=FALSE}

dat_o <- tibble(Weather = c("Rainy", "Rainy", "Sunny", "Sunny"),
                Response = c("No Smile","Smile","No Smile","Smile"),
                Counts = c(37,25,33,50)) %>% 
  pivot_wider(names_from = "Response", values_from = "Counts")

```

Here is our data:

```{r, echo=FALSE}
knitr::kable(dat_o, align = "c")
```

We are going to need to know the Column Totals and Row Totals and the Total number of participants (N), so lets calculate them add them to our tables:

-   **Rainy Row Total** = `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile")` + `r dat_o %>% filter(Weather == "Rainy") %>% pull("Smile")` = `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile") + dat_o %>% filter(Weather == "Rainy") %>% pull("Smile")`

-   **Sunny Row Total** = `r dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile")` + `r dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")` = `r dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")`

-   **No Smile Column Total** = `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile")` + `r dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile")` = `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile")`

-   **Smile Total** = `r dat_o %>% filter(Weather == "Rainy") %>% pull("Smile")` + `r dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")` = `r dat_o %>% filter(Weather == "Rainy") %>% pull("Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")`

-   **N =** `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile")` + `r dat_o %>% filter(Weather == "Rainy") %>% pull("Smile")` +`r dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile")` + `r dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")` = `r dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile") + dat_o %>% filter(Weather == "Rainy") %>% pull("Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")`

And if we add those to our table we see:

```{r, echo=FALSE}

colsums_chi <- tibble(Response = c("No Smile", "Smile"),
               sum = c(dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile"), dat_o %>% filter(Weather == "Rainy") %>% pull("Smile") + dat_o %>% filter(Weather == "Sunny") %>% pull("Smile"))) %>%
  pivot_wider(names_from = "Response",
              values_from = "sum") %>%
  mutate(Weather = "Totals") %>%
  select(Weather, `No Smile`, Smile)

dat <- dat_o %>%
  bind_rows(colsums_chi) %>%
  mutate(Totals = `No Smile` + Smile)

knitr::kable(dat, align = "c")
```

Now the formula for the chi-square is:

$$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$$

The Expected values for each condition, in the cross-tabulation in a few ways but the one we will use here is probably the easiest to use and it is:

$$Expected = \frac{Total_{row} \times Total_{column}}{N_{total}}$$

This is the same as other versions you might have seen such as:

$$Expected = \frac{Total_{row}}{N_{total}} \times \frac{Total_{column}}{N_{total}} \times N_{total}$$

The will both give the same result. So using the first approach we would see that the Expected values are:

**For people in the rain that did not smile:**

$$Expected_{Rainy-No Smile} = \frac{`r dat %>% filter(Weather == "Rainy") %>% pull("Totals")` \times `r dat %>% filter(Weather == "Totals") %>% pull("No Smile")`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = \frac{`r format(dat %>% filter(Weather == "Rainy") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("No Smile"), scientific = FALSE)`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = `r (dat %>% filter(Weather == "Rainy") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("No Smile"))/(dat %>% filter(Weather == "Totals") %>% pull("Totals"))`$$ **For people in the rain that did smile:**

$$Expected_{Rainy-Smile} = \frac{`r dat %>% filter(Weather == "Rainy") %>% pull("Totals")` \times `r dat %>% filter(Weather == "Totals") %>% pull("Smile")`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = \frac{`r format(dat %>% filter(Weather == "Rainy") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("Smile"), scientific = FALSE)`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = `r (dat %>% filter(Weather == "Rainy") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("Smile"))/(dat %>% filter(Weather == "Totals") %>% pull("Totals"))`$$

**For people in the sun that did not smile:**

$$Expected_{Sunny-No Smile} = \frac{`r dat %>% filter(Weather == "Sunny") %>% pull("Totals")` \times `r dat %>% filter(Weather == "Totals") %>% pull("No Smile")`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = \frac{`r format(dat %>% filter(Weather == "Sunny") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("No Smile"), scientific = FALSE)`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = `r (dat %>% filter(Weather == "Sunny") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("No Smile"))/(dat %>% filter(Weather == "Totals") %>% pull("Totals"))`$$

**For people in the sun that did smile:**

$$Expected_{Sunny-Smile} = \frac{`r dat %>% filter(Weather == "Sunny") %>% pull("Totals")` \times `r dat %>% filter(Weather == "Totals") %>% pull("Smile")`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = \frac{`r format(dat %>% filter(Weather == "Sunny") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("Smile"), scientific = FALSE)`}{`r dat %>% filter(Weather == "Totals") %>% pull("Totals")`} = `r (dat %>% filter(Weather == "Sunny") %>% pull("Totals") * dat %>% filter(Weather == "Totals") %>% pull("Smile"))/(dat %>% filter(Weather == "Totals") %>% pull("Totals"))`$$

We now have our data, let's start putting it into the formula, which we said was:

$$\chi^2 = \sum\frac{(Observed - Expected)^2}{Expected}$$

Which really means:

$$\chi^2 = \frac{(Observed_{Rainy-No Smile} - Expected_{Rainy-No Smile})^2}{Expected_{Rainy-No Smile}} + \frac{(Observed_{Rainy-Smile} - Expected_{Rainy-Smile})^2}{Expected_{Rainy-Smile}}$$

$$+ \\ \frac{(Observed_{Sunny-No Smile} - Expected_{Sunny-No Smile})^2}{Expected_{Sunny-No Smile}} +  \frac{(Observed_{Sunny-Smile} - Expected_{Sunny-No})^2}{Expected_{Sunny-No}}$$

```{r hide-we, echo=FALSE}
AYO <- dat_o %>% filter(Weather == "Rainy") %>% pull("No Smile")
ANO <- dat_o %>% filter(Weather == "Rainy") %>% pull("Smile")
BYO <- dat_o %>% filter(Weather == "Sunny") %>% pull("No Smile")
BNO <- dat_o %>% filter(Weather == "Sunny") %>% pull("Smile")

AYE <-(dat %>% filter(Weather == "Rainy") %>% pull(Totals) * dat %>% filter(Weather == "Totals") %>% pull("No Smile"))/(dat %>% filter(Weather == "Totals") %>% pull(Totals))
ANE <-(dat %>% filter(Weather == "Rainy") %>% pull(Totals) * dat %>% filter(Weather == "Totals") %>% pull("Smile"))/(dat %>% filter(Weather == "Totals") %>% pull(Totals))
BYE <-(dat %>% filter(Weather == "Sunny") %>% pull(Totals) * dat %>% filter(Weather == "Totals") %>% pull("No Smile"))/(dat %>% filter(Weather == "Totals") %>% pull(Totals))
BNE <-(dat %>% filter(Weather == "Sunny") %>% pull(Totals) * dat %>% filter(Weather == "Totals") %>% pull("Smile"))/(dat %>% filter(Weather == "Totals") %>% pull(Totals))
NT <- dat %>% filter(Weather == "Totals") %>% pull(Totals)

```

And if we start putting in the values, becomes:

$$\chi^2 = \frac{(`r AYO` - `r AYE`)^2}{`r AYE`}+ \frac{(`r ANO` - `r ANE`)^2}{`r ANE`}+\frac{(`r BYO` - `r BYE`)^2}{`r BYE`}+\frac{(`r BNO` - `r BNE`)^2}{`r BNE`}$$

And if we start to tidy those top halves up a little it becomes:

$$\chi^2 = \frac{(`r AYO - AYE`)^2}{`r AYE`}+ \frac{(`r ANO - ANE`)^2}{`r ANE`}+\frac{(`r BYO - BYE`)^2}{`r BYE`}+\frac{(`r BNO - BNE`)^2}{`r BNE`}$$

And now we square those top halves to give:

$$\chi^2 = \frac{`r (AYO - AYE)^2`}{`r AYE`} + \frac{`r (ANO - ANE)^2`}{`r ANE`} + \frac{`r (BYO - BYE)^2`}{`r BYE`} + \frac{`r (BNO - BNE)^2`}{`r BNE`} $$

And then divide the top halves by the bottom halves

$$\chi^2 = {`r ((AYO - AYE)^2)/AYE`} + {`r ((ANO - ANE)^2)/ANE`} + {`r ((BYO - BYE)^2)/BYE`} + {`r ((BNO - BNE)^2)/BNE`}$$

And then we sum them altogether to find:

$$\chi^2 = `r (((AYO - AYE)^2)/AYE) + (((ANO - ANE)^2)/ANE) + (((BYO - BYE)^2)/BYE) + (((BNO - BNE)^2)/BNE)` $$

Meaning that, rounded to two decimal places, we find $\chi^2 = `r ((((AYO - AYE)^2)/AYE) + (((ANO - ANE)^2)/ANE) + (((BYO - BYE)^2)/BYE) + (((BNO - BNE)^2)/BNE)) %>% round2(2)`$

**Degrees of Freedom**

The degrees of freedom for the cross-tabulation is calculated as:

$$df = (Rows - 1) \times (Columns - 1)$$

Which is read as **the number of Rows minus 1 times the number of Columns minus 1**. If we look at our original data again:

```{r df-orig, echo=FALSE}
chival <- ((((AYO - AYE)^2)/AYE) + (((ANO - ANE)^2)/ANE) + (((BYO - BYE)^2)/BYE) + (((BNO - BNE)^2)/BNE)) %>% round2(2)


knitr::kable(dat_o, align = "c")
```

Looking at the table we see we have `r dim(dat_o)[1]` rows and `r dim(dat_o)[2] - 1` columns of actual observed data (not looking at the titles and group names), so:

$$Rows - 1 = `r dim(dat_o)[1]` - 1 = `r dim(dat_o)[1] - 1`$$

And

$$Columns - 1 = `r dim(dat_o)[2]-1` - 1 = `r dim(dat_o)[2]-1 - 1`$$

Meaning that:

$$df = (Rows - 1) \times (Columns - 1)$$

which becomes:

$$df = (`r dim(dat_o)[1]` - 1) \times (`r dim(dat_o)[2]-1` - 1)$$

And reduces to:

$$df = (`r dim(dat_o)[1] - 1`) \times (`r dim(dat_o)[2]-1 - 1`)$$

leaving us with:

$$df = `r (dim(dat_o)[1] - 1) * ((dim(dat_o)[2]-1) - 1)`$$

so we see that $df = `r (dim(dat_o)[1] - 1) * ((dim(dat_o)[2]-1) - 1)`$

Applying the NHST framework, we have the same Chi-Square distribution as before and we have a range of critical values depending on the degrees of freedom. If you compare these values to the overview, they match up and demonstrate the underlying hand calculations. If you check back at the look-up table, our conclusion is still the association is statistically significant as the observed test statistic is greater than the critical value for $\alpha = .05$.

## Calculating in R

Now you understand the logic behind the test, we can demonstrate how you perform the cross-tabulation Chi-Square in R. As before, there are two main ways you can enter data, either as raw values of observations or a table of frequencies. We will demonstrate both processes to prepare you for any kind of categorical data you are working with.

### Wrangling a frequency table

If you are working with raw data, you first need to arrange your categories into a frequency table. To create our extended smile and weather design, its a slightly longer process to create the mock data.

```{r Chi Square 2x2 raw}

# Outline categories and counts 
categories <- c("Smile_Sunny", "Smile_Rainy", "No Smile_Sunny", "No Smile_Rainy")
counts <- c(50, 25, 33, 37)

Smile_weather <- data.frame(ID = 1:sum(counts), # 1 to the sum of the counts for unique observations
                     response = rep(categories, # Repeat each category 
                                    counts)) # By each count

# Separate response into two categorical variables
Smile_weather <- Smile_weather %>% 
  separate(col = response, # Split the response variable
           into = c("Response", "Weather"), # Into two variables of response and weather
           sep = "_") # Separate when it sees an underscore _ 

head(Smile_weather)

```

We created `r sum(counts)` rows of data, 50 for those smiling in the sun, 25 for smiling in the rain, 33 not smiling in the sun, and 37 not smiling in the rain. We had an additional step in the middle to prepare the data as we separated the original categorical variable into two separate variables. For each observations, we want to know what their responses was (smile or no smile) and what the weather was (sun or rain).

Now we have both variables, we can create a contingency table for the combination of observations in each category. We will use the `table()` function again, but this time select our two categorical variables to enter. We now have a frequency table for each combination of response and weather.

```{r Save 2x2 frequency table}

Smile_weather_frequency <- Smile_weather %>% 
  select(Response, Weather) %>% 
  table()

Smile_weather_frequency

```

### Enter data as a frequency table

Alternatively, you might start with a frequency table if you do not have access to the raw data. As for the one-sample Chi-Square, we can enter the values into a matrix and give the rows and columns informative names. Now we have the combination of two variables, its important you check the order you enter the values and the final frequency table is consistent with what you have intended.

```{r Smiles 2x2 frequency table}
Smile_weather_frequency <- matrix(c(37, 33, 25, 50), # Frequency values for each category combination
                           ncol = 2, # 2 columns, so the data are not in one column
                           byrow = TRUE) # Enter the values one row at a time

# For clarity, add names to each column
colnames(Smile_weather_frequency) <- c("Rainy", "Sunny")

# For clarity, add names to each row
rownames(Smile_weather_frequency) <- c("No Smile", "Smile")

Smile_weather_frequency
```

### Chi-Square function in R

Regardless of the method you enter the data, you should now have a frequency table of your two categorical variables. You can apply the same base R function as before to calculate a cross-tabulation Chi-Square test.

```{r Chi square 2x2}
chisq.test(Smile_weather_frequency, 
           correct = FALSE)
```

In the output, we have our Chi-Square test statistics, the degrees of freedom, and the *p*-value. In this scenario, we added the argument `correct = FALSE` as by default, R applies the Yates's correction for continuity. This strictly applies only to the 2x2 application of the cross-tabulation Chi-Square, so it does not always apply. It applies a correction which typically reduces the Chi-Square value but it has been criticised for being overly conservative. You can see its effect and change to the output by editing the argument to `correct = TRUE`.

Back to the non-corrected example, we do have a statistically significant association between the facial response and the weather:

```{r chi square smiles weather reproducible, echo=FALSE}

chi_square <- chisq.test(Smile_weather_frequency, 
                         correct = FALSE)

CQ_stat <- round(chi_square$statistic, 2)
CQ_df <- chi_square$parameter

CQ_pval <- round(chi_square$p.value, 3)
CQ_pval <- substr(as.character(CQ_pval), 2, 5)

```

$\chi^2$ (`r CQ_df`) = `r CQ_stat`, *p* = `r CQ_pval`.

We can reject the null hypothesis and it appears there is a different pattern of facial responses depending on the weather. You might have noticed the trend from the frequency table, but it is easier to interpret from a bar plot of the frequencies.

```{r smiles weather bar plot}
Smile_weather %>% 
  ggplot(aes(x = Weather, fill = Response)) + 
  geom_bar(position = "dodge") + 
  theme_minimal() + 
  scale_y_continuous(limits = c(0,50), breaks = seq(0, 50, 10), name = "Frequency") + 
  scale_x_discrete(name = "Weather During Observation") + 
  scale_fill_viridis_d(option = "E", begin = 0.5, end = 0.95) + # Colourblind friendly pallete 
  labs(title = "The Frequency of Whether Participants Returned a Smile or Not by the Weather.")
```

When the weather is rainy, people are more likely to not return a smile. However, when it is sunny, people are more likely to return a smile. This shows there is an association between the pattern of facial response and the weather during the observation.

Finally, we can check the observed values for our final assumption. To work as intended, the expected values must be higher than 5 again. If the expected value is less than 5 for one of the cells/categories, you will receive a warning that the results may not be accurate. We can check the expected values for our full example.

```{r C2 weather smile expected}
chisq.test(Smile_weather_frequency, 
           correct = FALSE)$expected
```

To demonstrate what the warning looks like, we will need to amend the data set so there are fewer than 5 expected values.

```{r C2 weather smile low expected}
Smile_weather_frequency <- matrix(c(4, 2, 5, 7), # Frequency values for each category combination
                           ncol = 2, # 2 columns, so the data are not in one column
                           byrow = TRUE) # Enter the values one row at a time

# For clarity, add names to each column
colnames(Smile_weather_frequency) <- c("Rainy", "Sunny")

# For clarity, add names to each row
rownames(Smile_weather_frequency) <- c("No Smile", "Smile")

chisq.test(Smile_weather_frequency, 
           correct = FALSE)

chisq.test(Smile_weather_frequency, 
           correct = FALSE)$expected
```

This shows how when you have fewer than 5 expected values, you receive the warning “Chi-squared approximation may be incorrect”. This highlights how the test statistic and *p*-value may be inaccurate when there is too little data to work with.

Now we have a 2x2 design, we can demonstrate the alternative test designed for smaller samples known as Fisher's exact test. It is another base R function called `fisher.test` and you simply enter the frequency table you should already have prepared.

```{r Fishers test}
fisher.test(Smile_weather_frequency)
```

The output is a little different to the standard Chi-Square. The key parts are the *p*-value and the odds ratio. The odds ratio tells you how many times more cases there are in one combination of variables compared to the other combination. For example, how many times more smile cases there are compared to no smile cases across the weather variable. An odds ratio of 1 means there is no difference. An odds ratio greater than 1 means there are more cases in your target condition, whereas an odds ratio less than 1 means are less cases in your target condition. This means its important to consider how your variables are organised.

As before, the *p*-value indicates whether you can reject the null hypothesis. The null hypothesis for this test is the odds ratio is 1: there is no difference in the number of cases across variables. The alternative hypothesis is the odds ratio is not 1: there *is* a difference in the number of cases across variables.

For this tiny example, assuming $\alpha = .05$, there is not a statistically significant effect and we cannot reject the null hypothesis.

# Conclusion

In this tutorial, you have worked through how to conduct and interpret the Chi-Square test in R. First, you applied the test to a single categorical variable, then to two or more categorical variables. For each application, you worked through an overview of how the test works, what the underlying calculations are, and how to wrangle and analyse the data in R. You also saw you can visualise categorical data using a bar plot to show the frequency of different categories.

You should now be able to apply these techniques to when you are working with categorical data of your own and answering your own research question.
