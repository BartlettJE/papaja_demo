
@article{hoffman_students_2021,
	title = {Do {Students} {Learn} {More} from {Erroneous} {Code}? {Exploring} {Student} {Performance} and {Satisfaction} in an {Error}-{Free} {Versus} an {Error}-full {SAS}® {Programming} {Environment}},
	volume = {0},
	issn = {null},
	shorttitle = {Do {Students} {Learn} {More} from {Erroneous} {Code}?},
	url = {https://www.tandfonline.com/doi/abs/10.1080/26939169.2021.1967229},
	doi = {10.1080/26939169.2021.1967229},
	abstract = {Teaching students statistical programming languages while simultaneously teaching them how to debug erroneous code is challenging. The traditional programming course focuses on error-free learning in class while students’ experiences outside of class typically involve error-full learning. While error-free teaching consists of focused lectures emphasizing correct coding, error-full teaching would follow such lectures with debugging sessions. We aimed to explore these two approaches by conducting a pilot study of 18 graduate students who voluntarily attended a SAS programming seminar held weekly from September 2018 through November 2018. Each seminar had a 10-min error-free lecture, 15-min programming assignment, 5-min break, 10-min error-full lecture, and 15-min programming assignment. We examined student performance and preference. While four students successfully completed both assignments and ten students did not successfully complete either assignment, one student successfully completed only the first assignment that directly followed the error-free lecture and three students successfully completed only the second assignment that directly followed the error-full lecture. Of the 15 students who responded, twelve (80\%) preferred error-full to error-free learning. We will evaluate error-full learning on a larger scale in an introductory SAS course. Supplemental files are available online for this article.},
	number = {0},
	urldate = {2021-09-23},
	journal = {Journal of Statistics and Data Science Education},
	author = {Hoffman, Heather J. and Elmi, Angelo F.},
	month = aug,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/26939169.2021.1967229},
	keywords = {Error-free learning, Error-full learning, SAS® software, Statistical programming},
	pages = {1--13},
	file = {Full Text PDF:C\:\\Users\\jamesb\\Zotero\\storage\\EFM8BP9N\\Hoffman and Elmi - 2021 - Do Students Learn More from Erroneous Code Explor.pdf:application/pdf;Snapshot:C\:\\Users\\jamesb\\Zotero\\storage\\584VJM6I\\26939169.2021.html:text/html},
}

@article{bebermeier_creating_2019,
	title = {Creating {Statistics} {Exercises} on the {Basis} of {Research} {Articles}},
	volume = {46},
	issn = {0098-6283, 1532-8023},
	url = {http://journals.sagepub.com/doi/10.1177/0098628319853938},
	doi = {10.1177/0098628319853938},
	abstract = {We describe how students can be encouraged to actively review course contents on inferential statistics by creating applicationoriented exercises and sample solutions on the basis of concrete and realistic research articles and their data. For evaluation purposes, we use students’ reactions to the activity and investigate its effects on the final statistics exam. Students’ ratings of the quality of the exercises, necessary knowledge, and the evaluation of the activity were very positive, and participating students achieved better grades in the final exam than nonparticipants, whereas the groups did not differ regarding their initial skills or motivational characteristics. We conclude by discussing implications for lecturers teaching statistics.},
	language = {en},
	number = {3},
	urldate = {2019-12-14},
	journal = {Teaching of Psychology},
	author = {Bebermeier, Sarah and Hagemann, Anne},
	month = jul,
	year = {2019},
	pages = {240--245},
	file = {0098628319853938.pdf:C\:\\Users\\jamesb\\Zotero\\storage\\F5GDN23F\\0098628319853938.pdf:application/pdf},
}

@article{simonsohn_small_2015,
	title = {Small {Telescopes}: {Detectability} and the {Evaluation} of {Replication} {Results}},
	volume = {26},
	issn = {0956-7976},
	shorttitle = {Small {Telescopes}},
	url = {https://doi.org/10.1177/0956797614567341},
	doi = {10.1177/0956797614567341},
	abstract = {This article introduces a new approach for evaluating replication results. It combines effect-size estimation with hypothesis testing, assessing the extent to which the replication results are consistent with an effect size big enough to have been detectable in the original study. The approach is demonstrated by examining replications of three well-known findings. Its benefits include the following: (a) differentiating ?unsuccessful? replication attempts (i.e., studies yielding p {\textgreater} .05) that are too noisy from those that actively indicate the effect is undetectably different from zero, (b) ?protecting? true findings from underpowered replications, and (c) arriving at intuitively compelling inferences in general and for the revisited replications in particular.},
	language = {en},
	number = {5},
	urldate = {2022-09-28},
	journal = {Psychological Science},
	author = {Simonsohn, Uri},
	month = may,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {559--569},
	file = {SAGE PDF Full Text:C\:\\Users\\jamesb\\Zotero\\storage\\EVXXVZK3\\Simonsohn - 2015 - Small Telescopes Detectability and the Evaluation.pdf:application/pdf},
}

@article{targ_meta-research_group_statistics_2022,
	title = {Statistics {Education} in {Undergraduate} {Psychology}: {A} {Survey} of {UK} {Curricula}},
	volume = {8},
	issn = {2474-7394},
	shorttitle = {Statistics {Education} in {Undergraduate} {Psychology}},
	url = {https://doi.org/10.1525/collabra.38037},
	doi = {10.1525/collabra.38037},
	abstract = {Graduates from psychology programmes are likely to use data skills throughout their career, regardless of whether they continue into research. Statistical education in psychology programmes, however, emphasizes inferential statistical tests over a deep understanding of data and data skills, which can lead to the problematic use and interpretation of statistics. Indeed, widely-used statistical practices appear to undermine the quality of scientific research and mislead end-users of data. Several proposals have been made for how to improve these practices—with effective statistical education being one. With this in mind, we sought to document the statistical content currently taught to undergraduate psychology students in the UK. Contrary to our expectations, we found that only 19\% of universities had publicly available curricula describing the statistical content taught in their undergraduate psychology programme. Of the curricula we obtained, most of them mentioned specific tests (ANOVAs, regression, correlation, t-tests, frequency tests, and rank tests) and about half mentioned probability and randomness, effect size, and statistical power, but few mentioned concepts such as confidence intervals, multiple comparisons, meta-analysis, replication, Bayesian statistics, frequentist statistics, and practical significance. These findings suggest that undergraduate psychology programmes may not emphasize statistical concepts (e.g., uncertainty) that are important for both everyday thinking and for effectively reporting and interpreting scientific research.},
	number = {1},
	urldate = {2022-11-04},
	journal = {Collabra: Psychology},
	author = {{TARG Meta-Research Group}},
	month = sep,
	year = {2022},
	pages = {38037},
	file = {Full Text PDF:C\:\\Users\\jamesb\\Zotero\\storage\\YIAPJPCR\\TARG Meta-Research Group - 2022 - Statistics Education in Undergraduate Psychology .pdf:application/pdf;Snapshot:C\:\\Users\\jamesb\\Zotero\\storage\\BD5VV3DY\\Statistics-Education-in-Undergraduate-Psychology-A.html:text/html},
}

@misc{mcaleer_embedding_2022,
	title = {Embedding {Data} {Skills} in {Research} {Methods} {Education}: {Preparing} {Students} for {Reproducible} {Research}},
	shorttitle = {Embedding {Data} {Skills} in {Research} {Methods} {Education}},
	url = {https://psyarxiv.com/hq68s/},
	doi = {10.31234/osf.io/hq68s},
	abstract = {Many initiatives to improve reproducibility incentivise replication and encourage greater transparency without directly addressing the underlying skills needed for transparent and reproducible data preparation and analysis. In this paper, we argue that training in data processing and transformation should be embedded in field-specific research methods curricula. Promoting reproducibility and open science requires not only teaching relevant values and practices, but also providing the skills needed for reproducible data analysis. Improving students’ data skills will also enhance their employability within and beyond the academic context. To demonstrate the necessity of these skills, we walk through the analysis of realistic data from a classic paradigm in experimental psychology that is often used in teaching: the Stroop Interference Task. When starting from realistic raw data, nearly 80\% of the data analytic effort for this task involves skills not commonly taught—namely, importing, manipulating, and transforming tabular data. Data processing and transformation is a large and inescapable part of data analysis, and so education should strive to make the work associated with it as efficient, transparent, and reproducible as possible. We conclude by considering the challenges of embedding computational data skills training in undergraduate programmes and offer some solutions.},
	language = {en-us},
	urldate = {2022-11-13},
	publisher = {PsyArXiv},
	author = {McAleer, Phil and Stack, Niamh and Woods, Heather and DeBruine, Lisa and Paterson, Helena and Nordmann, Emily and Kuepper-Tetzel, Carolina E. and Barr, Dale J.},
	month = nov,
	year = {2022},
	keywords = {open science, Social and Behavioral Sciences, statistics, research methods, pedagogy, Meta-science, reproducibility, Quantitative Methods, Statistical Methods},
	file = {Full Text PDF:C\:\\Users\\jamesb\\Zotero\\storage\\A7WL8Y96\\McAleer et al. - 2022 - Embedding Data Skills in Research Methods Educatio.pdf:application/pdf},
}
